{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444917c2-75be-4011-899f-dc1bb3c0bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2479c82-10d5-44f2-ac5f-6b89405d4b3b",
   "metadata": {},
   "source": [
    "## Co-ordinate Attention Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604e6b4f-d795-47aa-920d-5a28503fd9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordinateAttention(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, reduction=32):\n",
    "        super(CoordinateAttention, self).__init__()\n",
    "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
    "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
    "        self.reduce_channels = nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(in_channels // reduction)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.fc_h = nn.Conv2d(in_channels // reduction, out_channels, kernel_size=1)\n",
    "        self.fc_w = nn.Conv2d(in_channels // reduction, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        n, c, h, w = x.size()\n",
    "\n",
    "        # Apply horizontal pooling and transformations\n",
    "        x_h = self.pool_h(x).permute(0, 1, 3, 2)\n",
    "        x_h = self.reduce_channels(x_h).permute(0, 1, 3, 2)\n",
    "        x_h = self.bn(x_h)\n",
    "        x_h = self.fc_h(self.act(x_h))\n",
    "        x_h = x_h.sigmoid()\n",
    "\n",
    "        # Apply vertical pooling and transformations\n",
    "        x_w = self.pool_w(x)\n",
    "        x_w = self.reduce_channels(x_w)\n",
    "        x_w = self.bn(x_w)\n",
    "        x_w = self.fc_w(self.act(x_w))\n",
    "        x_w = x_w.sigmoid()\n",
    "\n",
    "        # Fusion of coordinate attention across height and width\n",
    "        out = identity * x_h * x_w\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace92f28-0f28-47df-8549-26977c19b722",
   "metadata": {},
   "source": [
    "## Multi-Scale Fusion Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe4cd284-5542-442a-937f-68695de26c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleFusionNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(MultiScaleFusionNetwork, self).__init__()\n",
    "\n",
    "        # Convolutions for different patch scales\n",
    "        self.conv_17 = nn.Conv3d(in_channels, 64, kernel_size=(1, 1, 17), padding=(0, 0, 8))\n",
    "        self.conv_19 = nn.Conv3d(in_channels, 64, kernel_size=(1, 1, 19), padding=(0, 0, 9))\n",
    "        self.conv_21 = nn.Conv3d(in_channels, 64, kernel_size=(1, 1, 21), padding=(0, 0, 10))\n",
    "\n",
    "        # Batch Normalization and Activation\n",
    "        self.bn_17 = nn.BatchNorm3d(64)\n",
    "        self.bn_19 = nn.BatchNorm3d(64)\n",
    "        self.bn_21 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Fusion layer to concatenate and combine the multi-scale features\n",
    "        self.fusion = nn.Conv3d(192, 128, kernel_size=1)  # 64 + 64 + 64 = 192 channels after concatenation\n",
    "        self.fusion_bn = nn.BatchNorm3d(128)\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * 145 * 145, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutions for different scales\n",
    "        x_17 = self.relu(self.bn_17(self.conv_17(x)))\n",
    "        x_19 = self.relu(self.bn_19(self.conv_19(x)))\n",
    "        x_21 = self.relu(self.bn_21(self.conv_21(x)))\n",
    "\n",
    "        # Concatenate the features from all scales\n",
    "        fused_features = torch.cat([x_17, x_19, x_21], dim=1)\n",
    "\n",
    "        # Apply fusion convolution and batch normalization\n",
    "        fused_features = self.relu(self.fusion_bn(self.fusion(fused_features)))\n",
    "\n",
    "        # Flatten the features and apply the final classification layer\n",
    "        fused_features = fused_features.view(fused_features.size(0), -1)\n",
    "        out = self.fc(fused_features)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37737602-9724-4ab3-8b93-fb65110c677a",
   "metadata": {},
   "source": [
    "## Model with Attention and Multi-scale Fusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7446ee7d-c40a-4886-96f9-6133159a5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperspectralClassificationModel(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, use_attention=True, use_multi_scale_fusion=True):\n",
    "        super(HyperspectralClassificationModel, self).__init__()\n",
    "\n",
    "        # First few layers: simple 3D convolutions\n",
    "        self.conv1 = nn.Conv3d(in_channels, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.bn2 = nn.BatchNorm3d(128)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Optional Coordinate Attention module\n",
    "        self.use_attention = use_attention\n",
    "        if self.use_attention:\n",
    "            self.coord_attention = CoordinateAttention(128, 128)\n",
    "\n",
    "        # Optional Multi-Scale Fusion module\n",
    "        self.use_multi_scale_fusion = use_multi_scale_fusion\n",
    "        if self.use_multi_scale_fusion:\n",
    "            self.multi_scale_fusion = MultiScaleFusionNetwork(in_channels=128, num_classes=num_classes)\n",
    "\n",
    "        # Fully connected layers for final classification\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * 145 * 145, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial convolutions\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "\n",
    "        # Coordinate Attention (if enabled)\n",
    "        if self.use_attention:\n",
    "            x = self.coord_attention(x)\n",
    "\n",
    "        # Multi-Scale Fusion (if enabled)\n",
    "        if self.use_multi_scale_fusion:\n",
    "            x = self.multi_scale_fusion(x)\n",
    "        else:\n",
    "            # Flatten the features and apply the final classification layer directly\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92fa384-0fc0-46d6-8dad-b0cf8524bab3",
   "metadata": {},
   "source": [
    "### Dataset Splitting and Training and Testing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2134222c-9c84-4a6d-a9f1-fd6e1605b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for Hyperspectral Images with Patches\n",
    "class HyperspectralDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data.astype(np.float16)  # Ensure data is of type float32\n",
    "        self.labels = labels.astype(np.int32)  # Ensure labels are of type int64\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patch = self.data[idx]  # Get the patch of data at index idx\n",
    "        label = self.labels[idx]  # Get the label at index idx\n",
    "        return torch.tensor(patch, dtype=torch.float16), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f10dec6-a7dc-4c12-8901-60d3253b7834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract patches from hyperspectral images\n",
    "def extract_patches(img, labels, patch_size):\n",
    "    margin = patch_size // 2\n",
    "    img_padded = np.pad(img, [(margin, margin), (margin, margin), (0, 0)], mode='constant')\n",
    "    \n",
    "    patches, patch_labels = [], []\n",
    "    for i in range(margin, img.shape[0] - margin):\n",
    "        for j in range(margin, img.shape[1] - margin):\n",
    "            if labels[i, j] != 0:  # Exclude background\n",
    "                patch = img_padded[i-margin:i+margin+1, j-margin:j+margin+1, :]\n",
    "                patches.append(patch)\n",
    "                patch_labels.append(labels[i, j] - 1)  # Convert to zero-based index\n",
    "    return np.array(patches), np.array(patch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a18ab16-b3f5-4d41-bc5f-d2dd455fe240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, num_classes):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(num_classes))\n",
    "    OA = accuracy_score(y_true, y_pred)\n",
    "    class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    class_acc[np.isnan(class_acc)] = 0  # Handle divide by zero for empty classes\n",
    "    AA = np.mean(class_acc)\n",
    "    Kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    return {\"OA\": OA, \"AA\": AA, \"Kappa\": Kappa}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10745245-23b5-43da-8002-f9ae098cc99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split Indian Pines data as per the provided class-wise splits\n",
    "def load_indian_pines_data_with_splits(patch_size):\n",
    "    # Load the Indian Pines dataset\n",
    "    data = sio.loadmat('Indian_pines_corrected.mat')['indian_pines_corrected']\n",
    "    labels = sio.loadmat('Indian_pines_gt.mat')['indian_pines_gt']\n",
    "    \n",
    "    # Define the per-class splits (Train, Val, Test)\n",
    "    splits = {\n",
    "        0: (8, 4, 34),\n",
    "        1: (105, 105, 1218),\n",
    "        2: (100, 100, 630),\n",
    "        3: (50, 50, 130),\n",
    "        4: (80, 80, 323),\n",
    "        5: (100, 100, 530),\n",
    "        6: (10, 4, 14),\n",
    "        7: (40, 40, 398),\n",
    "        8: (8, 4, 8),\n",
    "        9: (110, 110, 752),\n",
    "        10: (110, 110, 2235),\n",
    "        11: (100, 100, 393),\n",
    "        12: (40, 40, 125),\n",
    "        13: (100, 100, 1065),\n",
    "        14: (80, 80, 226),\n",
    "        15: (20, 10, 63)\n",
    "    }\n",
    "    \n",
    "    patches_train, labels_train = [], []\n",
    "    patches_val, labels_val = [], []\n",
    "    patches_test, labels_test = [], []\n",
    "    \n",
    "    # Extract patches and split into train, validation, and test sets\n",
    "    for class_label, (train_count, val_count, test_count) in splits.items():\n",
    "        class_indices = np.where(labels == class_label + 1)  # One-based indexing in labels\n",
    "        class_samples = list(zip(class_indices[0], class_indices[1]))\n",
    "        np.random.shuffle(class_samples)  # Shuffle indices for randomness\n",
    "        \n",
    "        for idx, (i, j) in enumerate(class_samples):\n",
    "            patch = extract_patches(data, labels, patch_size)[0]  # Extract patch\n",
    "            \n",
    "            if idx < train_count:\n",
    "                patches_train.append(patch)\n",
    "                labels_train.append(class_label)\n",
    "            elif idx < train_count + val_count:\n",
    "                patches_val.append(patch)\n",
    "                labels_val.append(class_label)\n",
    "            else:\n",
    "                patches_test.append(patch)\n",
    "                labels_test.append(class_label)\n",
    "    \n",
    "    # Convert lists to arrays\n",
    "    patches_train, labels_train = np.array(patches_train), np.array(labels_train)\n",
    "    patches_val, labels_val = np.array(patches_val), np.array(labels_val)\n",
    "    patches_test, labels_test = np.array(patches_test), np.array(labels_test)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = HyperspectralDataset(patches_train, labels_train)\n",
    "    val_dataset = HyperspectralDataset(patches_val, labels_val)\n",
    "    test_dataset = HyperspectralDataset(patches_test, labels_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bac90e1a-e0ec-472d-a25f-570e664d9d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function (similar to previous code)\n",
    "def train_model(train_loader, val_loader, in_channels, num_classes, epochs=20, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = HyperspectralClassificationModel(in_channels=in_channels, num_classes=num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.permute(0, 3, 1, 2).to(device), labels.to(device)  # [B, C, H, W]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.permute(0, 3, 1, 2).to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_accuracy = correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f711b413-168a-4743-9d0e-5bf40e21c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model and calculating metrics\n",
    "def test_model(model, test_loader, y_test, num_classes):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            inputs = inputs.permute(0, 3, 1, 2).to(device)  # [B, C, H, W]\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    metrics = calculate_metrics(y_test, y_pred, num_classes)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd5c4cd5-52cf-4bfb-a030-86d7f67a37d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating window size: 17x17\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1005. MiB for an array with shape (9119, 17, 17, 200) and data type uint16",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m patch_size \u001b[38;5;129;01min\u001b[39;00m patch_sizes:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating window size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     train_loader, val_loader, test_loader, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mload_indian_pines_data_with_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Initialize models for training\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     model1 \u001b[38;5;241m=\u001b[39m HyperspectralClassificationModel(\n\u001b[0;32m     12\u001b[0m         in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, \n\u001b[0;32m     13\u001b[0m         num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, \n\u001b[0;32m     14\u001b[0m         use_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[0;32m     15\u001b[0m         use_multi_scale_fusion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[8], line 38\u001b[0m, in \u001b[0;36mload_indian_pines_data_with_splits\u001b[1;34m(patch_size)\u001b[0m\n\u001b[0;32m     35\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(class_samples)  \u001b[38;5;66;03m# Shuffle indices for randomness\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (i, j) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(class_samples):\n\u001b[1;32m---> 38\u001b[0m     patch \u001b[38;5;241m=\u001b[39m \u001b[43mextract_patches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Extract patch\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m train_count:\n\u001b[0;32m     41\u001b[0m         patches_train\u001b[38;5;241m.\u001b[39mappend(patch)\n",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m, in \u001b[0;36mextract_patches\u001b[1;34m(img, labels, patch_size)\u001b[0m\n\u001b[0;32m     11\u001b[0m             patches\u001b[38;5;241m.\u001b[39mappend(patch)\n\u001b[0;32m     12\u001b[0m             patch_labels\u001b[38;5;241m.\u001b[39mappend(labels[i, j] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Convert to zero-based index\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(patches), np\u001b[38;5;241m.\u001b[39marray(patch_labels)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1005. MiB for an array with shape (9119, 17, 17, 200) and data type uint16"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    patch_sizes = [17, 19, 21]  # List of window sizes\n",
    "    results = {}  # Dictionary to store results for each model and patch size\n",
    "\n",
    "    for patch_size in patch_sizes:\n",
    "        print(f\"Evaluating window size: {patch_size}x{patch_size}\")\n",
    "\n",
    "        train_loader, val_loader, test_loader, y_test = load_indian_pines_data_with_splits(patch_size)\n",
    "\n",
    "        # Initialize models for training\n",
    "        model1 = HyperspectralClassificationModel(\n",
    "            in_channels=200, \n",
    "            num_classes=16, \n",
    "            use_attention=False, \n",
    "            use_multi_scale_fusion=False\n",
    "        )\n",
    "\n",
    "        model2 = HyperspectralClassificationModel(\n",
    "            in_channels=200, \n",
    "            num_classes=16, \n",
    "            use_attention=True, \n",
    "            use_multi_scale_fusion=False\n",
    "        )\n",
    "\n",
    "        model3 = HyperspectralClassificationModel(\n",
    "            in_channels=200, \n",
    "            num_classes=16, \n",
    "            use_attention=True, \n",
    "            use_multi_scale_fusion=True\n",
    "        )\n",
    "\n",
    "        # Train model 1\n",
    "        train_loss1, val_loss1, val_accuracy1 = train_model(train_loader, val_loader, model1, epochs=100, lr=3e-4)\n",
    "        results[f'Model 1 - {patch_size}x{patch_size}'] = {\n",
    "            'Train Loss': train_loss1,\n",
    "            'Validation Loss': val_loss1,\n",
    "            'Validation Accuracy': val_accuracy1\n",
    "        }\n",
    "\n",
    "        # Train model 2\n",
    "        train_loss2, val_loss2, val_accuracy2 = train_model(train_loader, val_loader, model2, epochs=100, lr=3e-4)\n",
    "        results[f'Model 2 - {patch_size}x{patch_size}'] = {\n",
    "            'Train Loss': train_loss2,\n",
    "            'Validation Loss': val_loss2,\n",
    "            'Validation Accuracy': val_accuracy2\n",
    "        }\n",
    "\n",
    "        # Train model 3\n",
    "        train_loss3, val_loss3, val_accuracy3 = train_model(train_loader, val_loader, model3, epochs=100, lr=3e-4)\n",
    "        results[f'Model 3 - {patch_size}x{patch_size}'] = {\n",
    "            'Train Loss': train_loss3,\n",
    "            'Validation Loss': val_loss3,\n",
    "            'Validation Accuracy': val_accuracy3\n",
    "        }\n",
    "\n",
    "        # Print the results for all models for the current patch size\n",
    "        for model_name, metrics in results.items():\n",
    "            if patch_size in model_name:  # Print only the results for the current patch size\n",
    "                print(f\"{model_name}:\")\n",
    "                print(f\"  Train Loss: {metrics['Train Loss']:.4f}\")\n",
    "                print(f\"  Validation Loss: {metrics['Validation Loss']:.4f}\")\n",
    "                print(f\"  Validation Accuracy: {metrics['Validation Accuracy']:.4f}\")\n",
    "\n",
    "        # Test each model and calculate metrics\n",
    "        for model_key in ['Model 1', 'Model 2', 'Model 3']:\n",
    "            model = eval(model_key.replace(\" \", \"\").lower())  # Use model1, model2, model3\n",
    "            metrics = test_model(model, test_loader, y_test, 16)\n",
    "            print(f\"Metrics for {model_key} at window size {patch_size}x{patch_size}: {metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99890c6-54a9-477b-9e15-423254e18065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cd3391-eb2d-4d0f-b912-bd9ad9f25a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
